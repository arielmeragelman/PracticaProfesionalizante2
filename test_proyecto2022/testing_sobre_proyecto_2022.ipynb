{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arielmeragelman/PracticaProfesionalizante2/blob/testing/test_proyecto2022/testing_sobre_proyecto_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea1de751",
      "metadata": {
        "id": "ea1de751"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc92fcce",
      "metadata": {
        "id": "bc92fcce",
        "outputId": "e357d7c1-15c8-49de-fbe1-941d21cfb6e8"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3750702446.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install PyMySQL\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234a79da",
      "metadata": {
        "id": "234a79da"
      },
      "outputs": [],
      "source": [
        "# librerias requeridas:\n",
        "# PyMySQL\n",
        "# pandas\n",
        "# BeautifulSoup\n",
        "\n",
        "#DEFINO LOS DATOS PARA LA CONEXION CON LA BASE DE DATO DONDE SE GUARDA LA INFORMACION DEL SCRAPPING\n",
        "\n",
        "import itertools\n",
        "import pymysql \n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parametros de conexion a bbdd\n",
        "link=pymysql.connect(\n",
        "   host=\"192.141.210.12\",    \n",
        "   user=\"laboratorio\", \n",
        "   password=\"laboratorio\",\n",
        "   db=\"bd_1\", \n",
        "   charset=\"utf8\",\n",
        "   port=3306\n",
        "   )\n",
        "\n",
        "def strdate_to_floatdate(data):\n",
        "#Funcion para convertir los indices de fecha en str a indices en float considerando la unidad el año y como decimales los meses (mes/12) y los dias (dias/30)\n",
        "    return int(data[0:4])+int(data[5:7])/120+int(data[8:10])/3000\n",
        "def regresion_lineal(x,y,j):\n",
        "    X_train=np.array(list(map(strdate_to_floatdate,x)))\n",
        "    fit = np.polyfit(X_train, y, deg=1)\n",
        "    print (\"Slope : \" + str(fit[0]))\n",
        "    print (\"Intercept : \" + str(fit[1]))\n",
        "    \n",
        "    \n",
        "    #Fit function : y = mx + c [linear regression ]\n",
        "    fit_function = np.poly1d(fit)\n",
        "\n",
        "    #Linear regression plot\n",
        "    plt.plot(x, fit_function(X_train))\n",
        "    #Time series data plot\n",
        "    plt.plot(x, Y_train)\n",
        "\n",
        "    plt.xlabel('Fecha')\n",
        "    plt.ylabel('Precio')\n",
        "    plt.title('Cluster N°'+str(j))\n",
        "    plt.show()\n",
        "    return fit_function\n",
        "\n",
        "\n",
        "def scrap(año, mes, url):\n",
        "    # funcion para hacer un web scrapping con el valor del dolar\n",
        "    \n",
        "    for i in range(1,7):\n",
        "        try:        \n",
        "            fecha = datetime.datetime(año,mes,i)\n",
        "            data = {'fecha': fecha.strftime('%d-%m-%y')}\n",
        "            resp = requests.post(url, data=data)\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            break\n",
        "        except:\n",
        "            print('Falló en ',i)    \n",
        "    filas = soup.find_all('td', {'style' : 'padding: 1%'})\n",
        "    return filas\n",
        "\n",
        "def parsear(filas):\n",
        "  # Funcion para convertir el formato de los datos \n",
        "    mensual = pd.DataFrame() \n",
        "    for i in range(1, int(len(list(filas))/3)):\n",
        "        dic = {}\n",
        "        dic['fecha'] = filas[3*i].text\n",
        "        dic['bid'] = filas[3*i+1].text\n",
        "        dic['ask'] = filas[3*i+2].text\n",
        "        rueda = pd.DataFrame.from_dict(dic, orient='index').transpose().set_index('fecha')\n",
        "        rueda.index = pd.to_datetime(rueda.index, format='%d-%m-%y ')\n",
        "        mensual = pd.concat([mensual,rueda], axis=0)\n",
        "    return mensual\n",
        "\n",
        "def downloadAño(año,url):\n",
        "    tablaAnual = pd.DataFrame()\n",
        "    for i in range(1,13):\n",
        "        filas = scrap(año=año, mes=i,url=url)\n",
        "        tabla = parsear(filas)\n",
        "        tablaAnual = pd.concat([tablaAnual,tabla],axis=0)\n",
        "        print('mes',i,'listo')        \n",
        "    tablaAnual.to_excel('blue_'+str(año)+'.xlsx')\n",
        "    print(tablaAnual)\n",
        "    return tablaAnual\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def analisis_kmean(matriz,k):\n",
        "        # GENERAMOS CENTROIDES PARA 5 CLUSTERS\n",
        "    kmeans = KMeans(n_clusters=k).fit(X)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    \n",
        "    \n",
        "    print(\"_\"*20)\n",
        "    print(\"-\"*20)\n",
        "    print(\"CENTROIDES PARA K = {}\".format(k))\n",
        "    print(centroids)\n",
        "    \n",
        "    \n",
        "    #vemos el representante del grupo, el usuario cercano a su centroid\n",
        "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X)\n",
        "    print(\"_\"*20)\n",
        "    print(\"-\"*20)\n",
        "    print(\"REPRESENTANTES DE CADA GRUPO DE K = {}\".format(k))\n",
        "    print(closest)\n",
        "    \n",
        "    \n",
        "    # Producto que representa a cada centroide\n",
        "    print(\"_\"*20)\n",
        "    print(\"-\"*20)\n",
        "    \n",
        "    n_centroides=[]\n",
        "    for row in closest:\n",
        "        print(Y[row])\n",
        "        n_centroides.append(Y[row])\n",
        "    PD_centroides= pd.DataFrame({'Productos':n_centroides})\n",
        "    \n",
        "    PD_centroides.to_csv(\"KMEAN_Centroides\"+str(k)+\"K_V1.csv\")\n",
        "\n",
        "        \n",
        "    \n",
        "    # Desarrollamos una lista de las categoria asociada a cada producto \n",
        "    categoria=[]\n",
        "    for i in range(0,matriz.shape[0]):\n",
        "        lugar=kmeans.predict( np.array( [ matriz.iloc[i,:] ]  ) )\n",
        "        lugar=list(lugar)[0]\n",
        "        categoria.append(lugar)\n",
        "    \n",
        "    PD_Categorias= pd.DataFrame({'Productos':matriz.index.values,'Categoria':categoria})\n",
        "    \n",
        "    PD_Categorias.to_csv(\"KMEAN_\"+str(k)+\"K_V1.csv\")\n",
        "    return PD_Categorias\n",
        "\n",
        "\n",
        "#DEFINO LA CONSULTA PARA LEER LA INFORMACION DE LA BASE DE DATOS\n",
        "\n",
        "\n",
        "def consulta_lineas(sql,link):\n",
        "  \n",
        "    cursor = link.cursor()\n",
        "    cursor.execute(sql)\n",
        "    print(cursor.rowcount, \"LECTURA CORRECTA\")\n",
        "    salida=[]\n",
        "    for row in cursor.fetchall():\n",
        "        print(row)\n",
        "        salida.append(row)\n",
        "    cursor.close()\n",
        "    return salida\n",
        "    \n",
        "\n",
        "#EJECUTO UNA CONSULTA MODELO PARA EL ANALISIS DE DATOS\n",
        "sql = \"\"\"select distinct N_Producto,Precio_Origen,FECHA from entradas_super where N_Super = \"SUPER MAMI\" order by N_Producto,Fecha ASC  \"\"\"\n",
        "#OBTENGO UNA LISTA CON LAS SALIDAS DE LA QUERY\n",
        "tabla=consulta_lineas(sql,link)\n",
        "\n",
        "# CONVIERTO LA LISTA EN UN DATAFRAME\n",
        "a= pd.DataFrame(tabla)\n",
        "\n",
        "\n",
        "# DEFINO DOS ARREGLOS UNIDIMENCIONALES CON LAS FECHAS Y LOS PRODUCTOS SIN QUE SE REPITAN\n",
        "nuevas_columnas=a[0].unique()\n",
        "fechas= a[2].unique()\n",
        "\n",
        "\n",
        "# CREO UNA MATRIZ QUE OBTIENE LOS VALORES DE LA \"tabla\"  PERO LOS ORDENA PONIENDO CADA ELEMENTO DE FORMA UNICA E IRREPETIBLE SEGUN EL NOMBRE DEL PRODUCTO Y LA FECHA\n",
        "# QUEDANDO LA FECHA COMO INDICE DE FILAS Y LOS PRODUCTOS COMO INDICE DE COLUMNAS\n",
        "\n",
        "matriz_nueva=[]\n",
        "for j,dia in enumerate(fechas):\n",
        "    matriz_nueva.append([])\n",
        "    for i,columna in enumerate(nuevas_columnas):\n",
        "            try:\n",
        "                valor=   float(a[ (a[0]==columna) & (a[2]==dia) ][1] )\n",
        "            except TypeError:\n",
        "                    valor=   0\n",
        "            \n",
        "           # print(\"En la posicion i:\"+str(i)+\", j: \"+str(j)+\" Tenemos el valor: \"+str(valor))\n",
        "            matriz_nueva[j].append(valor)\n",
        "            \n",
        "print(\"FINALIZADO\")\n",
        "\n",
        "#CONVIERTO A LA MATRIZ EN UN DATAFRAME\n",
        "matriz_nueva=pd.DataFrame(matriz_nueva)\n",
        "#DEFINO EL NOMBRE DE LAS COLUMNAS\n",
        "matriz_nueva.columns=nuevas_columnas\n",
        "#DEFINO LOS INDICES DE FILAS\n",
        "matriz_nueva.index=fechas\n",
        "#DEFINO EL NOMBRE DEL INDICE DE FILA\n",
        "matriz_nueva.index.name=\"fechas\"\n",
        "matriz_nueva.to_csv(\"resguardo_csv\")\n",
        "\n",
        "\n",
        "\n",
        "# OBTENGO 2 DATAFRAME CON LOS VALORES HISTORICOS DEL DOLAR EN ARGENTINA\n",
        "\n",
        "año=2022\n",
        "\n",
        "dolar_blue= downloadAño(año,'https://www.cotizacion-dolar.com.ar/dolar-blue-historico-'+str(año)+'.php')\n",
        "dolar_oficial= downloadAño(año,'https://www.cotizacion-dolar.com.ar/dolar-historico-bna-'+str(año)+'.php')\n",
        "\n",
        "dolar=pd.merge(dolar_blue, dolar_oficial, on='fecha')\n",
        "\n",
        "# Cambio el formato del index de la matriz obtenida por web scrapping para que el indice sea en formato datetime y poder unirlo con la matriz del precio del dolar\n",
        "matriz_nueva.index = pd.to_datetime(matriz_nueva.index)\n",
        "\n",
        "matriz_con_dolar=pd.merge(matriz_nueva, dolar, left_index=True, right_index=True, how='left')\n",
        "\n",
        "\n",
        "# CONVIERTO LOS OBJETOS EN VALORES FLOTANTES PARA PODER COMPARAR LA VARIACION\n",
        "\n",
        "for i in matriz_con_dolar.columns:\n",
        "       if matriz_con_dolar[i].dtype == 'O':\n",
        "            try:\n",
        "                matriz_con_dolar[i]=matriz_con_dolar[i].str.strip('$ ').astype(float)\n",
        "            except:\n",
        "                passmatriz_con_dolar\n",
        "\n",
        "\n",
        "# COMENZAMOS CON LA LIMPIEZA DE VALORES IRRACIONALES\n",
        "# CONVERTIMOS TODOS LOS PRECIOS INFERIORES A $2 EN NAN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for i in range(len(nuevas_columnas)):\n",
        "   \n",
        "    indice=nuevas_columnas[i]\n",
        "    matriz_con_dolar.loc[  matriz_con_dolar[indice] < 10, indice    ] = np.nan\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# COMPLETAMOS CON DATOS LOS CAMPOS VACIOS\n",
        "matriz_con_dolar=matriz_con_dolar.fillna(method=\"ffill\")\n",
        "matriz_con_dolar=matriz_con_dolar.fillna(method=\"backfill\")\n",
        "\n",
        "\n",
        "\n",
        "#Doble chequeamos los minimos para quitar posibles errores de scrapping\n",
        "for columna in matriz_con_dolar.columns:\n",
        "    \n",
        "    print(\"minimo de {}: {}\".format(columna,matriz_con_dolar[columna].min()))\n",
        "\n",
        "\n",
        "# ELIMINAMOS LAS COLUMNAS QUE ESTEN COMPLETAMENTE INUTILIZABLES\n",
        "nuevas_columnas=matriz_con_dolar.columns\n",
        "\n",
        "for v,i in enumerate(nuevas_columnas):\n",
        "    if matriz_con_dolar[i].isna().all() :\n",
        "        del matriz_con_dolar[i]\n",
        "\n",
        "\n",
        "# DEFINIMOS EL NUEVO ARREGLO DE COLUMNAS  CONSIDERANDO QUE HEMOS QUITADO ALGUNAS DE SU ARREGLO ORIGINAL\n",
        "\n",
        "nuevas_columnas=matriz_con_dolar.columns\n",
        "\n",
        "matriz_con_dolar=matriz_con_dolar.sort_index()\n",
        "\n",
        "# Creamos una copia para poder realizar pruebas sin dañar la informacion original\n",
        "matriz_con_dolar_bk=matriz_con_dolar.copy()\n",
        "\n",
        "\n",
        "\n",
        "# <<<<<----- OJO HAY QUE VERIFICAR QUE EL CONTROL DE DUPLICADO LO HAGA SI SE REPITEN LOS VALORES EN TODDA LA FILA\n",
        "\n",
        "# VAMOS A VERIFICAR SI HAY COLUMNAS QUE TENGAN LOS MISMOS PRECIOS EN TODAS LAS FILAS,\n",
        "#DE SER ASI SE PODRIA  TRATAR DE UN MISMO PRODUCTO CON DISTINTAS DENOMINACIONES Y QUE\n",
        "#NO APORTARA INFORMACION UTIL\n",
        "\n",
        "duplicados =[]\n",
        "\n",
        "for col in range(matriz_con_dolar.shape[1]):\n",
        "        contents= matriz_con_dolar.iloc[:, col]\n",
        "        \n",
        "        for comp in range( col + 1,matriz_con_dolar.shape[1]):\n",
        "            if contents.equals(matriz_con_dolar.iloc[:, comp]):\n",
        "                duplicados.append(comp)\n",
        "                \n",
        "duplicados = np.unique(duplicados).tolist()\n",
        "      \n",
        "matriz_con_dolar=matriz_con_dolar.drop( matriz_con_dolar.iloc[:, duplicados].columns,axis=1)\n",
        "\n",
        "#PUNTO DE RESGUARDO DE LOS DATOS\n",
        "matriz_con_dolar.to_csv(\"Matriz_precios_back.csv\")\n",
        "\n",
        "\n",
        "matriz_con_dolar=matriz_con_dolar.drop('indicator',axis=1)\n",
        "\n",
        "\n",
        "#  Genero una matriz de valores con la variacion de precios entre registros de una misma categoria\n",
        "matriz_variacion=(matriz_con_dolar/matriz_con_dolar.shift(1))-1\n",
        "\n",
        "# Eliminamos la primer fila ya que no tiene valores comparativos de utilidad\n",
        "matriz_variacion=matriz_variacion.iloc[1: , :]\n",
        "\n",
        "\n",
        "\n",
        "no_varian=[]\n",
        "for i in matriz_variacion:\n",
        "    if len(matriz_variacion[i].unique()) == 1:\n",
        "        print(\"SIN VARIACION DE PRECIOS \"+i)\n",
        "        no_varian.append(i)\n",
        "#print(\"NO varian: \"+str(varian))\n",
        "print(\"total productos: \"+str(len(matriz_variacion.columns)))\n",
        "\n",
        "\n",
        "#Eliminamos las columnas que no presentan variacion de precio\n",
        "\n",
        "print(matriz_variacion.shape)\n",
        "\n",
        "for i in no_varian:\n",
        "    try:\n",
        "        matriz_variacion = matriz_variacion.drop(columns=[i], axis = 1)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"Ya se eliminaron las columnas\")\n",
        "print(matriz_variacion.shape)\n",
        "\n",
        "matriz_variacion_acumulada= matriz_variacion.cumsum()\n",
        "\n",
        "# RESGUARDAMOS LOS DATOS\n",
        "matriz_variacion_v1 = matriz_variacion_acumulada.copy()\n",
        "# Eliminamos el indice de fechas para trabajar el modelo de datos como mediciones individuales y no como una serie de tiempo\n",
        "matriz_variacion_v1=matriz_variacion_v1.reset_index(drop=True)\n",
        "\n",
        "matriz_variacion_v1.head()\n",
        "\n",
        "matriz_variacion_v1.to_csv(\"matriz_variacion_v1.csv\")\n",
        "\n",
        "# Inicio clusterizacion de datos\n",
        "T_matriz_variacion_v1 = matriz_variacion_v1.T\n",
        "T_matriz_variacion_v1.to_csv(\"backup_matriz_variacion\")\n",
        "\n",
        "#Armamos el conjunto de datos para trabajar en el proceso de clusterizado\n",
        "Y=matriz_variacion_v1.index.values\n",
        "X=np.array(matriz_variacion_v1)\n",
        "\n",
        "kmeans = KMeans(n_clusters=6).fit(X)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X)\n",
        "# Creamos una lista \"categoria\" donde predecimos la categoria de todos los productos que tenemos disponibles\n",
        "categoria=[]\n",
        "for i in range(0,480):\n",
        "    lugar=kmeans.predict( np.array( [ matriz_variacion_v1.iloc[i,:] ]  ) )\n",
        "    lugar=list(lugar)\n",
        "    categoria.append(lugar)\n",
        "\n",
        "\n",
        "# Unimos en una sola lista a todos los productos\n",
        "\n",
        "categoria=list(itertools.chain(*categoria))\n",
        "print(categoria)\n",
        "\n",
        "# Creamos un Dataframe de dos columnas, producto y su categoria\n",
        "PD_Categorias_K6= pd.DataFrame({'Productos':matriz_variacion_v1.index.values,'Categoria':categoria})\n",
        "\n",
        "# Respaldamos en csv\n",
        "PD_Categorias.to_csv(\"KMEAN_6K_V1.csv\")\n",
        "\n",
        "analisis_kmean(matriz_variacion_v1,6)\n",
        "\n",
        "#cargamos los datos de entrada\n",
        "indice = pd.read_csv(\"KMEAN_Centroides6K_V1.csv\",index_col=[0])\n",
        "data = pd.read_csv(\"Matriz_precios_back.csv\",index_col=[0])\n",
        "\n",
        "centroides_k6=[]\n",
        "for i in range(indice.shape[0]): \n",
        "    centroides_k6.append(data[indice.iloc[i,:][0]])\n",
        "\n",
        "fit_f=[]\n",
        "for i in range(len(centroides_k6)):\n",
        "    X_train=np.array(centroides_k6[i].index)\n",
        "    Y_train=np.array(centroides_k6[i].values)\n",
        "    fit_f.append(regresion_lineal(X_train,Y_train,i))\n",
        "\n",
        "\n",
        "for i in range(len(centroides_k6)):\n",
        "    X_train=np.array(centroides_k6[i].index)\n",
        "\n",
        "    fit_function=fit_f[0]\n",
        "    prediction = fit_function(X_train.size + 14)\n",
        "    print(prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}